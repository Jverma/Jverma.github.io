<p>We often hear there are two schools of thought in statistics - Frequentist and Bayesian. A the very fundamental level the difference in these two approaches stems from the way they interpret probability. For a frequentist, probability is defined in terms of limiting frequency of occurrence of an event while a bayesian statistician defines probability as the degree of disbelief on the occurrence of an event. This post in not about the philosophical aspects of the debate. Rather we will study an example from frequentist and bayesian methods. The example we will consider is the linear regression model. 
<br /><br />
<strong>Setup:</strong>
 Let the data be \(( D= { x_i,y_i }_{1 \leq i \leq N} \) where each \( x_i  \in \mathbb{R}^n \) and \( y_i \in \mathbb{R} \).
The linear regression model predicts the values of \(( y_i \)’s as linear combinations of the features \(( x_i \)’s</p>

<script type="math/tex; mode=display">y_i =w_0 +\sum_j w_j x_{ij} =w_0 +w^T x_i</script>

<p>Adding 1 to the vectors \(( x_i \)) to redefine \(( x_i = (1,x_i) \)) and combining \(( w_0 \)) and \(( w_i \))’s into a single vector \(( w = (w_0, w_1, \ldots, w_n) \)), this can be written as</p>

<script type="math/tex; mode=display">y_ i = w^T x_i</script>

<p>The idea is to estimate the values of parameters \(( \hat{w} \)) from the training data and predict the \(( y \))-value for a new observation \(( \tilde{x} \)) as</p>

<p><script type="math/tex">\tilde{y} = \hat{w}^T \tilde{x}</script>
<br /><br />
We will consider <em>Maximum likelihood estimation</em> (Frequentist), <em>Maximum a Posteriori</em> (semi-bayesian) and <em>Bayesian regression models</em>. 
<br /><br />
<strong>tl;dr</strong>
MLE chooses the parameters which maximize the likelihood of data given that parameter, MAP chooses parameters which maximize the posterior probability  of that parameter in the light of observed data and Bayesian inference computes the posterior probability distribution for the parameters given data.
<br /><br />
<strong>Maximum Likelihood Estimation:</strong>
The observed values of \(( y_i \))  is assumed to have Gaussian noise error i.e.</p>

<script type="math/tex; mode=display">y_i = w^T x_i + \epsilon</script>

<p>where \(( \epsilon \sim N(0,\sigma^2) \)).</p>

<p>The Likelihood in this case is given by</p>

<script type="math/tex; mode=display">\mathcal{L}(D | w, \sigma) = (2 \pi \sigma^2)^{-n/2} \prod_{i=1}^{n} exp \left[ \frac{-(y_i - \hat{y}_i)^2}{2\sigma^2} \right]</script>

<p>then the log-likelihood is given by</p>

<script type="math/tex; mode=display">\ln(\mathcal{L}) = - \frac{n}{2} \ln(2\pi \sigma^2) -  \frac{1}{2\sigma^2} \sum_i (y_i - w^T x_i)^2</script>

<p>Now MLE states that the estimated value of \(( w \)) is given by</p>

<script type="math/tex; mode=display">w_{MLE} = argmax_w \ln(\mathcal{L}(D | w, \sigma))</script>

<p>which in this case reduces to</p>

<p><script type="math/tex">w_{MLE} = argmin_w \sum_i  (y_i - w^T x_i)^2</script>
<br />
This is why the linear regression model is often known as <em>least square method</em>. 
Now we differentiate with respect to $latex w$ and equate the derivative to zero to get the estimate of $latex w$. This can be more clearly expressed in terms of linear algebraic quantities.
 <br />
If \(( X = (x_1, x_2, \ldots, x_N)^T \)), \(( Y = (y_1, y_2, \ldots, y_N)^T \)) and  \(( \theta = (w_0, w_1, w_2, \ldots, w_N)^T \)), then one can check that the MLE for \(( \theta \)) is</p>

<script type="math/tex; mode=display">\hat{\theta} = (X^T X)^{-1} X^T Y</script>

<p>Similarly  \(( \sigma^2 \)) can be estimated by differentiating the MLE with respect to  \(( \sigma^2 \)) and equation the derivative to zero. 
<br /><br />
<strong>Maximum a Posteriori estimation:</strong>
 For MAP, we assume a Gaussian prior on \(( w \)) i.e 
<script type="math/tex">w \sim N(0, \lambda^{-1} I)</script></p>

<script type="math/tex; mode=display">P(w) = (\frac{\lambda}{2 \pi})^{n/2} exp \left[ -\frac{\lambda}{2} w^T w \right]</script>

<p>Then the posterior probability after we observe the training data is computed by Bayes rule as</p>

<script type="math/tex; mode=display">P(w|D) = \frac{P(w) P(D|w)}{P(D)}</script>

<p>as with MLE, we will maximize the log-posterior probability and then</p>

<script type="math/tex; mode=display">w_{MAP} = argmax_w \ln(P(w|d))</script>

<p>which reduces to</p>

<script type="math/tex; mode=display">w_{MAP} = argmin_w \sum_i (y_i - w^T x_i)^2 + \frac{\lambda}{2} w^T w</script>

<p>Thus, the MAP estimation can be thought of as regression with regularization. 
The MAP estimate in this case is</p>

<script type="math/tex; mode=display">w_{MAP} = (\lambda I + X^T X)^{-1} X^T y</script>

<p><br /><br />
<strong>Bayesian Regression:</strong>
 In Bayesian regression, the Bayesian philosophy is applied. Both MLE and MAP are point estimates but in Bayesian regression, we look for predictive probability. Here we make predictions by integrating over the posterior distribution of the model parameters $latex w$. 
If \(( \tilde{x} \)) is a new point, we compute the probability of \(( \tilde{y} \)), the y-value corresponding to this x is given by</p>

<script type="math/tex; mode=display">P(\tilde{y} | \tilde{x}, D, \sigma^2, \lambda) = \int P(\tilde{y} | w, \tilde{x}, \sigma^2 ) P(w|D, \sigma^2 , \lambda) dw</script>

<p>These integration are often very hard to to do analytically and rely on sophisticated MCMC methods. 
 <br />
In full Bayesian regression, we assume a prior on \(( \sigma^2 \)) in addition to prior on \(( w \)).</p>

<!-- **Further:**
	<li><a href="https://januverma.wordpress.com/2013/10/09/logistic-regression/">Logistic regression</a></li>
	<li><a href="https://januverma.wordpress.com/2013/10/15/regularization-in-logistic-regression/">Regularization</a></li>
	<li><a href="https://januverma.wordpress.com/2013/11/06/ad-ctr-prediction-using-logistic-regression/">CTR prediction @Google</a></li> -->
