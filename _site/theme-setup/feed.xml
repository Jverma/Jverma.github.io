<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
<title type="text">Random Inferences</title>
<generator uri="https://github.com/jekyll/jekyll">Jekyll</generator>
<link rel="self" type="application/atom+xml" href="http://jverma.github.io//feed.xml" />
<link rel="alternate" type="text/html" href="http://jverma.github.io/" />
<updated>2015-12-11T18:35:08-05:00</updated>
<id>http://jverma.github.io//</id>
<author>
  <name>Janu Verma</name>
  <uri>http://jverma.github.io//</uri>
  
</author>


  

<entry>
  <title type="html"><![CDATA[Bloom Filters]]></title>
  <link rel="alternate" type="text/html" href="http://jverma.github.io//bloom-filter/" />
  <id>http://jverma.github.io//bloom-filter</id>
  <published>2015-11-05T00:00:00-05:00</published>
  <updated>2015-11-05T00:00:00-05:00</updated>
  <author>
    <name>Janu Verma</name>
    <uri>http://jverma.github.io/</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;A &lt;strong&gt;Bloom filter&lt;/strong&gt; is a compact data structure which is used to test membership of an element in a set. It is built by constructing a probablistic representation of the set, which can be queried for membership. A BF is designed for speed and effciency. The trade off for this efficiency is that a Bloom filter is probabilistic, in the sense that it tells us that the element is either definitely not in the set or may be in the set. Thus we can have some false positives i.e. the elements which are not in the set but the BF computes postive probability of it being in the set.
&lt;br&gt;&lt;br&gt;
A bloom filter supports two operations&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Add&lt;/strong&gt;: adds an element \( x \) to the set \( S \).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Test &lt;/strong&gt;: checks whether a given element \( x \) is in the set \( S \) or not. Test returns a boolean value as follows &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$ Test = \begin{cases}
        false, \text{then x is definitely not in S} \
        true, \text{then x is probably in the set }
    \end{cases}
$$
&lt;br&gt;
A BF should also compute the &lt;em&gt;false positive rate&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Construction of a Bloom filter:&lt;/strong&gt;
&lt;br&gt;
Consider a set \( S = {a_1, a_2, \ldots, a_n} \). Bloom filters describe membership information of \( S \) using a bit array \( V \) of length \( m \). It consists of an array of \( m \) bits, each initialized to \( 0 \). To add an item \( \theta \) to the bloom filter, \( k \) independent hash functions \( H_{1}(\theta), H_{2}(\theta), \ldots, H_{k}(\theta) \) are calculated. Each maps \( \theta \) to an integer in \( [0, m) \) and the corresponding \( h \) array bits are set to 1.
&lt;br&gt;&lt;br&gt;
To test if an item \( \alpha \) is a member, the same hash functions are applied to 
\( \alpha \) and the corresponding values in the bitarray are checked. \( \alpha \) is a member if all corresponding bits are set to 1.
&lt;br&gt;&lt;br&gt;
By construction, a bloom filter correctly identifies whether a element is added to to it. But there is a false positive rate as well. A false positive occurs if all the bits corresponding to an element \( \gamma \) are set to 1 by coincidence because of the items besides \( \gamma \) that were added previously.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Example: &lt;/strong&gt;
&lt;br&gt;
I borrowed the following example from &lt;a href=&quot;http://kellabyte.com/2013/01/24/using-a-bloom-filter-to-reduce-expensive-operations-like-disk-io/&quot;&gt;kellabyte&lt;/a&gt;
&lt;br&gt;
A bloom starts by initializing an empty bitarray : 
&lt;figure&gt;
&lt;a href=&quot;https://januverma.files.wordpress.com/2015/06/bloomfilter_empty_thumb1.jpg&quot;&gt;&lt;img src=&quot;https://januverma.files.wordpress.com/2015/06/bloomfilter_empty_thumb1.jpg?w=300&quot; alt=&quot;bloomfilter_empty_thumb1&quot; width=&quot;300&quot; height=&quot;42&quot; class=&quot;alignnone size-medium wp-image-883&quot; /&gt;&lt;/a&gt;
&lt;fugure&gt;
&lt;br&gt;&lt;br&gt;
When a new element is added, the hashes are computed and the corresponding bits are turned on :
&lt;figure&gt;&lt;br&gt;
&lt;a href=&quot;https://januverma.files.wordpress.com/2015/06/bloomfilter_adding_thumb1.jpg&quot;&gt;&lt;img src=&quot;https://januverma.files.wordpress.com/2015/06/bloomfilter_adding_thumb1.jpg?w=300&quot; alt=&quot;bloomfilter_adding_thumb1&quot; width=&quot;300&quot; height=&quot;129&quot; class=&quot;alignnone size-medium wp-image-882&quot; /&gt;&lt;/a&gt;
&lt;figure&gt;
&lt;br&gt;&lt;br&gt;
When we query for an element, the same hashes are evaluated : 
&lt;figure&gt; 
&lt;a href=&quot;https://januverma.files.wordpress.com/2015/06/bloomfilter_querying_thumb1.jpg&quot;&gt;&lt;img src=&quot;https://januverma.files.wordpress.com/2015/06/bloomfilter_querying_thumb1.jpg?w=300&quot; alt=&quot;bloomfilter_querying_thumb1&quot; width=&quot;300&quot; height=&quot;196&quot; class=&quot;alignnone size-medium wp-image-881&quot; /&gt;&lt;/a&gt;
&lt;figure&gt;
&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Optimizing a bloom filter:&lt;/strong&gt;
&lt;br&gt;
There is a tradeoff between the size of the bitarray and the false positive rate. 
Note that after inserting \( n \) entries into a bloom filter of size \( m \) with \( k \) hash functions, the probability that a particular bit is still 0 is given by -
&lt;br&gt;
$$ P_0 = (1 - \frac{1}{m})^{kn} \simeq 1 - e^{-\frac{kn}{m}} $$
&lt;br&gt;
Hence the probability of a false positive i.e. the probability that all k-bits have been previously set is - 
&lt;br&gt;
$$ P_{err} = (1 - P_0)^{k} \simeq (1 - e^{-\frac{kn}{m}})^{k} $$
&lt;br&gt;
Given \( n \) and \( m \), the optimal number of hash functions that minimizes the false positive rate is -
&lt;br&gt;
$$ k_{optimal} \simeq \frac{m}{n} \ln 2 $$
&lt;br&gt;
In practice, only a small number of hash functions are used to reduce the computational overhead each additional hash function. 
&lt;br&gt;&lt;br&gt;
Usually we have a fair idea of how many elements will be added to the bloom filter, in such a case, we have choose the size of the bitarray keeping the number of hash functions reasonable. 
&lt;br&gt;&lt;br&gt;
e.g choosing \( m = 8n \) i.e. 1 byte per entry, and \( k=5 \), gives a false positive rate of \( 2.16\% \)
&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Implementation:&lt;/strong&gt;
&lt;br&gt;
Let&amp;#39;s try to implement a bloom filter in python following the above construction procedure. We will use python libraries &lt;em&gt;bitarray&lt;/em&gt; and &lt;em&gt;hashlib&lt;/em&gt;.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;hashlib&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;bitarray&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bitarray&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We will write a class called &lt;em&gt;BloomFilter&lt;/em&gt;. This takes two arguments - size of the bitarray \( m \) and number of hashes \( k \). &lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;BloomFilter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;   Implements a bloom filter.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;   &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bfArray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bitarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;br&gt;
As we discussed above a bloom filter computes \( k \) hashes for each element. Let&amp;#39;s create these hash functions using python library &lt;em&gt;hashlib&lt;/em&gt; which comes with base package. &lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;hash_functions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;   Build k hash functions.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;   Map key to an integer in [0,m).&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;   &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hashlib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;md5&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hexdigest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;.&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hexdigest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;divmod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;yield&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;br&gt;
Now we are ready to add elements to the bloom filter. &lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;element&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;   Add an element.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;   &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hash_functions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bfArray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;br&gt;
The following method will check for membership.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;contains&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;element&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;   Checks if an element is present in the bloom filter.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;   &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bfArray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hash_functions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;br&gt;
This completes our implementation of a bloom filter. Surely one can add more methods e.g. to get the number of elements added into the bloom filter or to compute the false positive rate. I&amp;#39;ll leave that to the reader. &lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Additional resources: &lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://billmill.org/bloomfilter-tutorial/&quot;&gt;Bloom filter examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.michaelnielsen.org/ddi/why-bloom-filters-work-the-way-they-do/&quot;&gt;Why bloom filters work&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://gsd.di.uminho.pt/members/cbm/ps/dbloom.pdf&quot;&gt;Scalable bloom filters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/jaybaird/python-bloomfilter&quot;&gt;python-bloomfilter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

    &lt;p&gt;&lt;a href=&quot;http://jverma.github.io//bloom-filter/&quot;&gt;Bloom Filters&lt;/a&gt; was originally published by Janu Verma at &lt;a href=&quot;http://jverma.github.io/&quot;&gt;Random Inferences&lt;/a&gt; on November 05, 2015.&lt;/p&gt;
  </content>
</entry>


  

<entry>
  <title type="html"><![CDATA[Community Detection in networks]]></title>
  <link rel="alternate" type="text/html" href="http://jverma.github.io//community-in-graphs/" />
  <id>http://jverma.github.io//community-in-graphs</id>
  <published>2015-10-14T00:00:00-04:00</published>
  <updated>2015-10-14T00:00:00-04:00</updated>
  <author>
    <name>Janu Verma</name>
    <uri>http://jverma.github.io/</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;An important question about a social, biological or economical network is to identify &lt;em&gt;communities&lt;/em&gt;. A &lt;strong&gt;community&lt;/strong&gt; is a subset of nodes which are &amp;#39;more&amp;#39; connected to each other than the members of the other subsets. There are various methods to do this, we will use &lt;a href=&quot;http://en.wikipedia.org/wiki/Girvan%E2%80%93Newman_algorithm&quot;&gt;&lt;strong&gt;Girvan-Newman algorithm&lt;/strong&gt;. &lt;/a&gt;
&lt;br&gt;&lt;br&gt;
Consider the following graph G (forgive poor visuals!)
&lt;figure&gt;
&lt;a href=&quot;https://januverma.files.wordpress.com/2014/08/graph.png&quot;&gt;&lt;img class=&quot;alignnone  wp-image-259&quot; src=&quot;http://januverma.files.wordpress.com/2014/08/graph.png?w=300&quot; alt=&quot;graph&quot; width=&quot;448&quot; height=&quot;340&quot; /&gt;&lt;/a&gt;
&lt;/figure&gt;
G has 7 nodes labeled A,B,C,D,E,F,G and there are edges between some of them. We want to extract communities in the graph.
&lt;br&gt;&lt;br&gt;
GN algorithm works by finding the edges which are &amp;#39;between&amp;#39; communities and then progressively removing these edges from the original graph.
&lt;br&gt;&lt;br&gt;
How do we decide which edges are most likely to be between communities ?
&lt;br&gt;&lt;br&gt;
Girvan and Newman defined the &lt;strong&gt;edge betweenness&lt;/strong&gt; of an edge \( (u,v) \) as the number of shortest paths between pairs of vertices that run along it i.e number of nodes \( x \) and \( y \) such that the edge \( (u,v) \) lies on the shortest path between \( x \) and \( y \).
&lt;br&gt;
This extends the notion of node betweenness to edges.
&lt;br&gt;&lt;br&gt;
If there are more than one shortest path between \( x \) and \( y \), then the contribution of \( (x,y) \) to the betweenness of \( (u,v) \) is equal to the fraction of the shortest paths that include \( (u,v) \).
&lt;br&gt;&lt;br&gt;
If a network contains communities that are only loosely connected by a few inter-community edges, then all shortest paths between different communities must go along one of these few edges. Thus, the edges connecting communities will have high edge betweenness. By removing these edges, we separate groups from one another and so reveal the underlying community structure of the graph.
&lt;br&gt;&lt;br&gt;
The algorithm&amp;#39;s steps for community detection are -&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The betweenness of all existing edges in the network is calculated first.&lt;/li&gt;
&lt;li&gt;The edge with the highest betweenness is removed.&lt;/li&gt;
&lt;li&gt;The betweenness of all edges affected by the removal is recalculated.&lt;/li&gt;
&lt;li&gt;Steps 2 and 3 are repeated until some stopping criterion is fulfilled.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br&gt;&lt;br&gt;
Let&amp;#39;s apply GN algorithm on our graph. For our graph (( G \), we can calculate the betweenness for all the edges -&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;AB: 5&lt;/li&gt;
&lt;li&gt;AC: 1&lt;/li&gt;
&lt;li&gt;BC: 5&lt;/li&gt;
&lt;li&gt;BD: 12&lt;/li&gt;
&lt;li&gt;DE: 4.5&lt;/li&gt;
&lt;li&gt;DG: 4.5&lt;/li&gt;
&lt;li&gt;EF: 1.5&lt;/li&gt;
&lt;li&gt;DF: 4&lt;/li&gt;
&lt;li&gt;GF: 1.5&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;&lt;br&gt;
As we can see in the graph that there are two shortest path between E and G, one going through D and the other through F. Thus each of the edges DE, EF, DG, GF are credited with half a shortest path.
&lt;br&gt;
Clearly the edge BD has the highest betweenness. So removing leaves us with two communities.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://januverma.files.wordpress.com/2014/08/graph1.png&quot;&gt;&lt;img class=&quot;alignnone  wp-image-267&quot; src=&quot;http://januverma.files.wordpress.com/2014/08/graph1.png?w=300&quot; alt=&quot;graph1&quot; width=&quot;484&quot; height=&quot;368&quot; /&gt;&lt;/a&gt;
&lt;br&gt;&lt;br&gt;
If we interpret the graph as a friendship network in a class, then A,B,C and D,E,F,G are two groups of friends. BD is the only interlocutor!
&lt;br&gt;
We can go further and re-calculate the betweenness for all the edges in the graph and remove edges accordingly.
&lt;br&gt;&lt;br&gt;
An implementation of GN algorithm in python can be found &lt;a href=&quot;https://github.com/Jverma/TextGraphics/blob/master/TextGraphics/Analysis/communityDetection.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;http://jverma.github.io//community-in-graphs/&quot;&gt;Community Detection in networks&lt;/a&gt; was originally published by Janu Verma at &lt;a href=&quot;http://jverma.github.io/&quot;&gt;Random Inferences&lt;/a&gt; on October 14, 2015.&lt;/p&gt;
  </content>
</entry>


  

<entry>
  <title type="html"><![CDATA[What do physicists mean by field theory]]></title>
  <link rel="alternate" type="text/html" href="http://jverma.github.io//qft/" />
  <id>http://jverma.github.io//qft</id>
  <published>2015-08-03T00:00:00-04:00</published>
  <updated>2015-08-03T00:00:00-04:00</updated>
  <author>
    <name>Janu Verma</name>
    <uri>http://jverma.github.io/</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Quantum_field_theory&quot;&gt;Quantum field theories (QFTs)&lt;/a&gt; are the main objects of study for a theoretical physicist, more so for those specializing in high energy physics. The standard model of particle physics is built on the framework of QFTs. Each of the fundamental particle (fermion or boson) is described by a QFT.
&lt;br&gt;&lt;br&gt;
These theories are extremely important from a mathematical perspective. They find applications in topology of low-dimensional manifolds, representation theory, curve counting in enumerative geometry, symplectic geometry, algebraic and complex geometry etc. A complete mathematical understanding of quantum field theory is not available at present. Mathematicians (following &lt;a href=&quot;http://www.math.upenn.edu/~blockj/scfts/segal.pdf&quot;&gt;Segal&lt;/a&gt;, &lt;a href=&quot;http://www.numdam.org/item?id=PMIHES_1988__68__175_0&quot;&gt;Atiyah&lt;/a&gt;) have been successful in obtaining a rigorous definition of &lt;a href=&quot;http://ncatlab.org/nlab/show/topological+quantum+field+theory&quot;&gt;topological quantum field theories (TQFTs)&lt;/a&gt;, a special class of QFTs.
&lt;br&gt;&lt;br&gt;
Even though we don&amp;#39;t understand QFTs, we can define &amp;quot;fields&amp;quot; which are the main ingredients of a field theory. In this post, we will define the notion of fields in classical theory. This theory is then quantized to obtain quantum field theory.
&lt;br&gt;
The setup for a field theory is :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Spacetime&lt;/strong&gt; - A smooth, finite dimensional manifold \( M \). 
An example of \( M \) is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Minkowski_space&quot;&gt;Minkowaski space&lt;/a&gt; \( M^{1,n} \). Often \( M \) is equipped with additional topological and geometric structures e.g metric, orientation, spin structure etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fields&lt;/strong&gt; - Variables of the field theory, in most cases a space of fields \( \mathcal{F} \)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;
We will take the following working definition of a (classical) field.
&lt;br&gt;
&lt;strong&gt;Definition:&lt;/strong&gt; A &lt;em&gt;field&lt;/em&gt; is a smooth &lt;a href=&quot;http://en.wikipedia.org/wiki/Section_%28fiber_bundle%29&quot;&gt;section&lt;/a&gt; of a &lt;em&gt;fiber bundle&lt;/em&gt; over \( M \)
&lt;br&gt;&lt;br&gt;
Let \( \pi : E \rightarrow M \) be a fiber bundle over \( M \), then a field is a smooth map
&lt;br&gt;
$$ \phi : M \rightarrow E $$
&lt;br&gt;
such that \( \pi(\phi(x)) = x \) for all \( x \in M \). We denote this as -
&lt;br&gt;
$$ \phi \in C^{\infty}(E) $$
&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Examples:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A &lt;strong&gt;real scalar field&lt;/strong&gt; is a smooth real-valued function on \( M \).
&lt;br&gt;
$$ \phi : M \rightarrow \mathbb{R} $$
This is a section of trivial fiber bundle with fibers isomorphic to \( \mathbb{R} \).&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;complex scalar field&lt;/strong&gt; is a smooth complex-valued function on the spacetime.
$$ \phi : M \rightarrow \mathbb{C} $$&lt;/li&gt;
&lt;li&gt;More generally, we can consider \( X \)- valued smooth function, where \( X \) is another smooth manifold.
$$ \Phi : M \rightarrow X $$
Such fields arise in &lt;strong&gt;sigma models&lt;/strong&gt;. The \( \sigma \) - models, with target manifold \( X \) a &lt;em&gt;Calabi-Yau&lt;/em&gt; or a &lt;em&gt;Kahler manifold&lt;/em&gt;, are of particular interest to mathematicians.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gauge theory&lt;/strong&gt; - The setup for a gauge theory is a &lt;em&gt;principal bundle&lt;/em&gt; with a &lt;em&gt;connection&lt;/em&gt;.
Let \( \pi : P \rightarrow M \) be a principal \( G \) - bundle.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;
&lt;strong&gt;Defintion:&lt;/strong&gt; A &lt;em&gt;connection&lt;/em&gt; \( \mathcal{A} \) on \( P \) is a \( \mathfrak{g} \) - valued 1-form, where \( \mathfrak{g} \) is the &lt;em&gt;Lie algebra&lt;/em&gt; of the &lt;em&gt;Lie group&lt;/em&gt; \( G \).
&lt;br&gt;
$$ \mathcal{A} \in C^{\infty}(P, T^{*}P) \otimes \mathfrak{g} $$
&lt;br&gt;
We call \( \mathcal{A} \) the &lt;em&gt;gauge field&lt;/em&gt; with gauge group \( G \). The Maxwell&amp;#39;s theory of electromagnetism is given by a circle bundle over the Minkowaski space where the electromagnetic potential is the connection.
&lt;br&gt;
In case of &lt;a href=&quot;http://en.wikipedia.org/wiki/Yang%E2%80%93Mills_theory&quot;&gt;Yang-Mills theory&lt;/a&gt;, the gauge group is \( SU(n) \) and thus the fields are the connections on the \( SU(n) \)- principal bundles over the spacetime manifold.
- &lt;strong&gt;Spinor fields&lt;/strong&gt; - These theories are more complex than the others and require more structure as they encode the behavior of fermions. Let $latex M$ be equipped with a spinor structure, and let
$$ \pi : S \rightarrow M $$
be a &lt;em&gt;spinor bundle&lt;/em&gt; on \( M \). Then the spinor fields are the sections of \( S \).
$$ \psi : M \rightarrow S $$
- &lt;strong&gt;Gravity&lt;/strong&gt; - Let \( M \) be a &lt;em&gt;Riemannian manifold&lt;/em&gt;. The fields in the theory of gravity, gravitational fields, are the &lt;em&gt;metrics&lt;/em&gt; on \( M \). A metric \( g \) is a symmetric \( (0,2) \)-forms on \( M \).
$$ g : M \rightarrow Symm^{(0,2)}(M) $$
- &lt;strong&gt;Supergravity&lt;/strong&gt; - The &lt;em&gt;Rarita-Schwinger field&lt;/em&gt; in supergravity theories, which describe a spin-\( 3/2 \) field, is given by a section of the &lt;em&gt;twisted spinor bundle&lt;/em&gt;.
$$  \psi : M \rightarrow S \otimes T^{*}M $$&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;br&gt;
In all the field theories, there exists a natural &lt;em&gt;evaluation map&lt;/em&gt;,
&lt;br&gt;
$$ eval : \mathcal{F} \times M \rightarrow E $$
given by
&lt;br&gt;
$$  (\phi, x) \rightarrow \phi(x) $$
This post is full of mathematical (differential geometric, to be more accurate) jargon. The idea was to give a quick introduction of field theory to mathematicians (geometers). &lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;http://jverma.github.io//qft/&quot;&gt;What do physicists mean by field theory&lt;/a&gt; was originally published by Janu Verma at &lt;a href=&quot;http://jverma.github.io/&quot;&gt;Random Inferences&lt;/a&gt; on August 03, 2015.&lt;/p&gt;
  </content>
</entry>


  

<entry>
  <title type="html"><![CDATA[How GATK Haplotypecaller works]]></title>
  <link rel="alternate" type="text/html" href="http://jverma.github.io//gatk/" />
  <id>http://jverma.github.io//gatk</id>
  <published>2015-07-21T00:00:00-04:00</published>
  <updated>2015-07-21T00:00:00-04:00</updated>
  <author>
    <name>Janu Verma</name>
    <uri>http://jverma.github.io/</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;a href=&quot;https://www.broadinstitute.org/gatk/&quot;&gt;Genome Analysis Toolkit (GATK)&lt;/a&gt; is one of the most popular bioinformatics softwares which provides functionality to perform variant discovery and genotyping. It has a strong emphasis on data quality assurance as well. The &lt;a href=&quot;https://www.broadinstitute.org/gatk/gatkdocs/org_broadinstitute_gatk_tools_walkers_haplotypecaller_HaplotypeCaller.php&quot;&gt;HaplotypeCaller&lt;/a&gt; is the main variant calling algorithm in GATK. It calls SNPs and indels simultaneously using local de novo assembly and a Bayesian statistical model. The HalpotypeCaller algorithm works in following steps : 
&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Find Active Regions:&lt;/strong&gt;
&lt;br&gt; 
First step in HalplotypeCaller procedure is to find the regions of high activity. These are the genomic regions which have strong evidence for variation. Each position is assigned a raw &lt;strong&gt;activity score&lt;/strong&gt; which is the probability that the position contains a variant. This probability is calculated using the &lt;strong&gt;reference-confidence model&lt;/strong&gt; (see Appendix). The raw profile thus obtained is then smoothened by copying the activity score over to the adjacent regions and then spreading out using a Gaussian kernel. Finally, the active regions are obtained as the ones containing positions with high-enough activity score.
Mathematically, for each position \( i \)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Assign score \( P(i,variant) \) which is the probability of \( i \) being a variant.&lt;/li&gt;
&lt;li&gt;For each \( j \) in the interval around \( i \) of radius \( r \leq 50bp \) 
$$ P(j,variant) = P(j,variant) + P(i,variant)$$ 
Here \( r \) is equal to the number of high quality soft-clipped bases that immediately follow or precede \( i \).&lt;/li&gt;
&lt;li&gt;The profile score is spread out using Gaussian kernel upto 50bp. The score of \( i \) after spreading is denoted \( score_i \)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;&lt;br&gt;
Then the active regions are computed as follows :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Final profile score of site i is given by 
$$ FinalScore_i = \sum_{k}score_k $$
where \( k \) runs over all the positions which contribute to the score of i as in previous step.&lt;/li&gt;
&lt;li&gt;Cut the genome at points where the final profile score goes from active to non-active regions by choosing a activity threshold.&amp;lt;&lt;/li&gt;
&lt;li&gt;Trimm the not-so-important bps from the regions obtained in the previous step.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Local assembly of the active regions:&lt;/strong&gt;
&lt;br&gt;
In this step the active regions are re-assembled de novo by building a &lt;a href=&quot;http://jverma.github.io//debruijn-graphs/&quot;&gt;de Bruijn&lt;/a&gt; type graph of the reference genome.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Build a deBruijn graph \( (G) \) of the reference sequence.&lt;/li&gt;
&lt;li&gt;Initialize a weight hashtable \( (W) \) with edges as keys with weight 0.&lt;/li&gt;
&lt;li&gt;Build a hashtable \( (UniqNodes) \) of unique k-mers (nodes) of the graph with their positions in the graph.&lt;/li&gt;
&lt;li&gt;For each read, look for its k-mers in \( UniqNodes \). If for a k-mer, there is a match and the \( k−1 \)-mer is in \( W \), increase the weight of the edge by 1. If a k-mer is not in the hashtable, we append it to the hashtable and add a new node in \( G \).&lt;/li&gt;
&lt;li&gt;Prune the graph by throwing out the paths in the graph with weights below a threshold.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Identify Haplotypes:&lt;/strong&gt; 
&lt;br&gt;
After the re-assembly graph is constructed and pruned, the halpotypes are extracted as :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;For each edge \( e \) in the graph, let \( i \)be the source vertex of \( e \). Compute transition probability of \( e \).
$$ Prob(e) = W[e]/OutDegree(i) $$
where \( W[e] \) is the weight of the edge and \( OutDegree(i) \) is the out degree of \( i \).&lt;/li&gt;
&lt;li&gt;For each path \( p \) in the graph, compute its likelihood score as 
$$ Lik(p) = \prod_{e \in p} Prob(e) $$ 
The product is over all the edges in the path \( p \).&lt;/li&gt;
&lt;li&gt;Select \( N \) paths with the highest likelihood scores. These are our potential Haplotypes.&lt;/li&gt;
&lt;li&gt;Align each halpotype to the reference genome and compute a &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2723002/&quot;&gt;CIGAR&lt;/a&gt; string for the haplotype.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Compute haplotype likelihoods:&lt;/strong&gt; 
&lt;br&gt;
In the previous step, we computed a set of potential halpotypes. This set of sites gives a super-set of what will eventually be called-variants. The next step in GATK pipeline is to compute the evidence of existence of a haplotype given the data. For each haplotype H,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Align each data read against H using &lt;strong&gt;&lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2766791/&quot;&gt;PairHMM&lt;/a&gt;&lt;/strong&gt; and compute the likelihood of observing the read given the halpotype. Thus we obtain \( P(read|H) \)for all reads.&lt;/li&gt;
&lt;li&gt;Mariginalize the per-read likelihoods of halpotypes over alleles to get the per-read likelihoods for each allele, \( P(read|A) \) for each allele A and read. For a given site, we list all the alleles observed in the data. Then, for each read, we look at the haplotypes that support each allele; we select the haplotype that has the highest likelihood for that read, and we write that likelihood in the new table. And that’s it! For a given allele, the total likelihood will be the product of all the per-read likeli be the variants.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Bayesian Genotyper:&lt;/strong&gt; 
&lt;br&gt;
GATK employed a Bayesian model to compute the most likely genotype of each sample at each site. The idea is to estimate the likelihoods of each possible genotype and predict the one with highest likelihood. The model is expressed by the following equation (&lt;a href=&quot;http://en.wikipedia.org/wiki/Bayes%27_theorem&quot;&gt;Bayes theorem&lt;/a&gt;) 
&lt;br&gt;
$$ P (G|D) = P(G)P(D|G) / \sum_i P(Gi)P(D|Gi) $$
&lt;br&gt;
here \( P(G) \) is the prior probability of the genotype \( G \). GATK uses a flat prior. Now the likelihood \( P(D|G) \) can be expressed as 
&lt;br&gt;
$$ P(D|G) = \prod_j \left( \frac{P(D_j|H_1)}{2} + \frac{P(D_j|H_2)}{2} \right) $$
&lt;br&gt;
where \( G = H_{1}H_{2} \) i.e. there are exactly two haplotypes (&lt;em&gt;diploid assumption&lt;/em&gt;). What remains to be figured out is \( P(D_{j}|H_{k}) \), which is the per-read likelihood of the of the haplotype \( H_k \) aggregated over all the reads supporting the halpotype.This is exactly what we computed in the previous step. We assign genotype as :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Compute \( P(G_{i}|D) \) for all possible genotypes \( G_i \).&lt;/li&gt;
&lt;li&gt;The predicted genotype is the one with maximum likelihood i.e. 
$$ argmax_{G_{i}} [P(Gi|D)] $$ &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Appendix - Reference Confidence Model:&lt;/strong&gt; 
&lt;br&gt;
The reference confidence model computes the probability of occurrence of a variant at each position on the genome.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Align the reads to the reference genome.&lt;/li&gt;
&lt;li&gt;At each position, estimate the probability that some non-reference allele is segregating at that position.&lt;/li&gt;
&lt;li&gt;The estimate of the probability that there is a variant at position i is given by : 
$$ P(i, variant) = \text{Number of reads with reference base}/\text{Number of reads with non-reference base} $$ &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Implementation:&lt;/strong&gt; 
&lt;br&gt;
A BASH script which implements the GATK variant calling pipeline using HaplotypeCaller can be found &lt;a href=&quot;https://github.com/Jverma/GATK-pipeline&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;http://jverma.github.io//gatk/&quot;&gt;How GATK Haplotypecaller works&lt;/a&gt; was originally published by Janu Verma at &lt;a href=&quot;http://jverma.github.io/&quot;&gt;Random Inferences&lt;/a&gt; on July 21, 2015.&lt;/p&gt;
  </content>
</entry>


  

<entry>
  <title type="html"><![CDATA[de Bruijn graphs and genomic assembly]]></title>
  <link rel="alternate" type="text/html" href="http://jverma.github.io//debruijn-graphs/" />
  <id>http://jverma.github.io//debruijn-graphs</id>
  <published>2015-05-16T00:00:00-04:00</published>
  <updated>2015-05-16T00:00:00-04:00</updated>
  <author>
    <name>Janu Verma</name>
    <uri>http://jverma.github.io/</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;Current genome sequencing technology (especially NGS) can sequence only short fragments of DNA. Generally, many copies of a region are sequenced to reduce the sequencing errors. The date from such sequencers, which is in form of short DNA sequence, needs to be assembled to obtain the complete genome. One of the modern methods to assemble the reads data is to use de Bruijn graphs.
&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Definition:&lt;/strong&gt; Given a sequence of characters, it’s &lt;strong&gt;k−dimensional de Bruijn graph&lt;/strong&gt; is defined as follows :
- The nodes are all the subsequences of length k, called &lt;em&gt;k-mers&lt;/em&gt;.
- An edge between two nodes is the subsequence of length k + 1 which has the first node as prefix and the second as suffix.
&lt;br&gt;&lt;br&gt;
e.g. For S = AGATAC, the 3-de Bruijn of S will have&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;nodes - AGA,GAT,ATA,TAC&lt;/li&gt;
&lt;li&gt;edges - AGAT,GATA,ATAC&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Observation:&lt;/strong&gt; There exist \( n^k \) k-mers in an alphabet containing n characters.
&lt;br&gt;
This implies that for English alphabet, we have 17576 3-mers. For DNA sequences the alphabet consists of 4 nucleotides A,G,T,C and thus there are 64 possible 3-mers.
&lt;br&gt;
A k-mer can appear more than one times in a de Bruijn graph with edges to different nodes.
&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Definition:&lt;/strong&gt; A &lt;strong&gt;Hamiltonian path&lt;/strong&gt; in a graph is a path that travels to every nodes exactly once.In the above example, a Hamiltonian path is given by
&lt;br&gt;
AGA -&amp;gt; GAT -&amp;gt; ATA -&amp;gt; TAC.
&lt;br&gt;&lt;br&gt;
It is easy to convince yourself that a Hamiltonian path in a de-Brujn graph constructed from the reads data will be a candidate for genome assembly because it visits each detected k-mer. Moreover, it will be the minimum length assembly as it travels to each node exactly one.
Unfortunately, there is no efficient algorithm to compute the Hamiltonian path in a large graph. In fact,
&lt;br&gt;
&lt;strong&gt;Theorem:&lt;/strong&gt; Finding a Hamiltonian path in a graph is NP-Complete.
&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Definition:&lt;/strong&gt; An &lt;strong&gt;Eulerian path&lt;/strong&gt; in a graph is a path that visits each edge exactly once. In the above example, an Eulerian path is given by
&lt;br&gt;
AGAT -&amp;gt; GATA -&amp;gt; ATAC
&lt;br&gt;&lt;br&gt;
An Eulerian path in a de Bruijn graph constructed on the sequence reads will also give a reasonable genome assembly as it visits each detected k + 1-mer. On the question of existence of an Eulerian path, we have following theorem of Euler -
&lt;br&gt;
&lt;strong&gt;Theorem:&lt;/strong&gt; A directed connected graph (one there exists a path between any two nodes) has an Eulerian cycle if and only if the indegree (number of edges leading into the node) of each node is equal to its outdegree (number of edges leaving the node).
&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Lemma:&lt;/strong&gt; de Bruijn graph has an Eulerian cycle.
&lt;br&gt;
&lt;strong&gt;proof:&lt;/strong&gt; For any node, both the indegree and the outdegree are equal to the number of times the k-mer assigned node occurs in the sequence.
&lt;br&gt;&lt;br&gt;
Having established that the de Bruijn graph has an Eulerian cycle, the next question is whether we can compute it efficiently. There are algorithms to compute the Eulerian cycle of a graph which are linear in number of edges.
&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Algorithm to assmeble genome:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Choose the size of k-mers.&lt;/li&gt;
&lt;li&gt;Generate all k-mers from the reads data.&lt;/li&gt;
&lt;li&gt;Build a de Bruijn graph with k-mers as nodes and connected (k + 1)-mers as edges.&lt;/li&gt;
&lt;li&gt;Compute the Eulerian path of the de Bruijn graph- This will be a candidate assmebly.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;&lt;br&gt;
Below is an example of genome assembly using de Bruijn graphs - 
&lt;br&gt;&lt;br&gt;
&lt;figure&gt;
&lt;img src=&quot;https://januverma.files.wordpress.com/2014/11/nbt-2023-f3.gif?w=300&quot;/&gt;
&lt;br&gt;&lt;br&gt;
Most of the times, sequencing doesn’t give us all the data. We have a lot of missing data which represent gaps in the genome and in that case, it would be impossible to obtain a whole genome assembly. So we build many de Bruijn graphs and obtain Eulerian cycles. Such cycles represent contiguous sequences of nucleotides, called &lt;strong&gt;contigs&lt;/strong&gt;. Sometimes we join contigs separated by gaps to obtain &lt;strong&gt;scaffolds&lt;/strong&gt;.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;http://jverma.github.io//debruijn-graphs/&quot;&gt;de Bruijn graphs and genomic assembly&lt;/a&gt; was originally published by Janu Verma at &lt;a href=&quot;http://jverma.github.io/&quot;&gt;Random Inferences&lt;/a&gt; on May 16, 2015.&lt;/p&gt;
  </content>
</entry>


  

<entry>
  <title type="html"><![CDATA[Gibbs Sampling]]></title>
  <link rel="alternate" type="text/html" href="http://jverma.github.io//gibbs-sampling/" />
  <id>http://jverma.github.io//gibbs-sampling</id>
  <published>2015-03-24T00:00:00-04:00</published>
  <updated>2015-03-24T00:00:00-04:00</updated>
  <author>
    <name>Janu Verma</name>
    <uri>http://jverma.github.io/</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Gibbs_sampling&quot;&gt;Gibbs sampling&lt;/a&gt; is a &lt;a href=&quot;http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo&quot;&gt;Markov chain Monte Carlo&lt;/a&gt; method for sampling from a multivariate probability distribution. It has many applications in inference, &lt;a href=&quot;http://en.wikipedia.org/wiki/Bayesian_network&quot;&gt;Bayesian networks&lt;/a&gt; and machine learning. I am a bioinformatician, who needs to sample from joint distributions all the time. What motivated me to write this post is that currently I&amp;#39;m building a Bayesian network for my work, where I need to draw samples from the network. I&amp;#39;ll talk about the use of Gibbs sampling in BayesNets in another post.&lt;/p&gt;

&lt;p&gt;Let&amp;#39;s start with a simple sampling problem. Suppose we have a single variable \( X \) which takes two values:
&lt;br&gt;&lt;br&gt;
\( P(X =0)=0.5 \) and \( P(X =1)=0.5 \)
&lt;br&gt;&lt;br&gt;
This is an example of &lt;a href=&quot;http://en.wikipedia.org/wiki/Binomial_distribution&quot;&gt;binomial distribution&lt;/a&gt;. How can get a sample from this distribution?
Simply, flip a coin. If it&amp;#39;s head, \( X=1 \), else \( X=0 \). The following python code will generate samples from this distribution.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sampler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;br&gt;
What if you have a &lt;a href=&quot;http://en.wikipedia.org/wiki/Multinomial_distribution&quot;&gt;multinomial distribution&lt;/a&gt; ? Say you want to model the roll of a dice:
&lt;br&gt;
$$ P(X = i) = 1/6 ,  i \in { 1,\ldots ,6 } $$
&lt;br&gt;
Divide the interval [0, 1] into 6 equal parts and select the value of \( X \) based on the interval in which a pseudo-random number falls.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sampler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;br&gt;
Suppose we have a multivariate distribution -
&lt;br&gt;
$$ P(X_1,X_2, \dots,X_n) $$
&lt;br&gt;
If the variables are independent of each other (i.e &lt;a href=&quot;http://en.wikipedia.org/wiki/Independence_(probability_theory)&quot;&gt;independence assumption&lt;/a&gt;, then we have
&lt;br&gt;
$$ P(X_1,X_2, \ldots,X_n) = \prod_{i=1}^{n} P(X_i) $$
&lt;br&gt;
which will be easy to sample. Draw samples from each of the \( P(X_i) \) individually and then multiply to get a sample of the joint distribution.
&lt;br&gt;
People who work with machine learning and data mining would recognize this assumption to be true for &lt;a href=&quot;http://en.wikipedia.org/wiki/Naive_Bayes_classifier&quot;&gt;Naive Bayes algorithm&lt;/a&gt;.
&lt;br&gt;&lt;br&gt;
But if the independent assumption doesn&amp;#39;t hold, the sampling problem is much harder. Gibbs sampling gives a procedure to sample from a joint probability distribution in terms of conditionals. This efficient method works under one condition:
&lt;br&gt;
&lt;em&gt;It is easy to sample from conditional distributions&lt;/em&gt; 
$$ P(X_i | X_1,X_2, \ldots, X_{i-1},X_{i+1}, \ldots X_n) $$ 
&lt;em&gt;for all&lt;/em&gt; \( X_i \)
&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Algorithm:&lt;/strong&gt;
&lt;br&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;￼1. Specify an initial value \( x^{(0)} = (x_1 \ldots ,x_n) \)&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Iterate for \( j = 1,2,3,\ldots \)&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;Pick an index \( i \) for \( 1 \leq i \leq n \) uniformly at random.&lt;/li&gt;
&lt;li&gt; Sample \( x_i \) from \( P(X_{i} | x^{(j-1)}_{(-i)}) \)&lt;/li&gt;
&lt;li&gt;\( x^{(j)} = (x^{(j-1)}_{(-i)}, x_{i}) \)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;&lt;br&gt;
The above sampling methodology generates a sequence of samples \( x^{(0)},x^{(1)},\ldots \). 
&lt;p&gt;
Intuitively, the relation between joint and conditional distributions play an important role in this approximation of joint distribution by conditionals.&lt;br&gt;
&lt;br&gt;
$$ P(X_i | X_1,X_2, \ldots, X_{i-1},X_{i+1}, \ldots X_n) = \frac{P(X_1,X_2, \ldots,X_n)}{P(X_1,X_2, \ldots, X_{i-1},X_{i+1}, \ldots X_n)} $$
&lt;br&gt;
More theoretically, it can be shown that this sequence forms a &lt;a href=&quot;http://en.wikipedia.org/wiki/Markov_chain&quot;&gt;Markov Chain&lt;/a&gt; over all the possible states. The &lt;em&gt;stationary state&lt;/em&gt; of this Markov chain is the sought after joint distribution \( P(X_1,X_2, \ldots,X_n) \). I&amp;#39;ll not go into the theory in this post, a curious soul can refer to other resources.
&lt;br&gt;&lt;br&gt; 
Gibbs sampling is a special case of the &lt;a href=&quot;http://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm&quot;&gt;Metropolis-Hastings algorithm&lt;/a&gt;.
&lt;br&gt;&lt;br&gt;
A nice implementation of the basic idea of Gibbs sampling can be found on &lt;a href=&quot;http://darrenjw.wordpress.com/2011/07/16/gibbs-sampler-in-various-languages-revisited/&quot;&gt;Darren Wilkinsen&amp;#39;s blog&lt;/a&gt;. &lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;http://jverma.github.io//gibbs-sampling/&quot;&gt;Gibbs Sampling&lt;/a&gt; was originally published by Janu Verma at &lt;a href=&quot;http://jverma.github.io/&quot;&gt;Random Inferences&lt;/a&gt; on March 24, 2015.&lt;/p&gt;
  </content>
</entry>


  

<entry>
  <title type="html"><![CDATA[Twitter Sentiment Analysis]]></title>
  <link rel="alternate" type="text/html" href="http://jverma.github.io//twitter-sentiment-analysis/" />
  <id>http://jverma.github.io//twitter-sentiment-analysis</id>
  <published>2015-02-24T00:00:00-05:00</published>
  <updated>2015-02-24T00:00:00-05:00</updated>
  <author>
    <name>Janu Verma</name>
    <uri>http://jverma.github.io/</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;Twitter represents a fundamentally new instrument to make social measurements. Millions of people voluntarily express opinions across any topic imaginable - this data source is incredibly valuable for both research and business. This means we can use the vast amount of data from Twitter to generate &amp;quot;public opinion&amp;quot; towards certain topics by aggregating the individual tweet results over time. 
&lt;p&gt;
Sentiment Analysis aims to determine how a certain person or group reacts to a specific topic. For Twitter, it works by extracting tweets containing references to the desired topic, computing the sentiment polarity and strength of each tweet and then aggregating the results for all such tweets.&lt;br&gt;
&lt;/p&gt;
We can also track changes in the users opinion towards these topics over time, allowing us to identify the events that caused these changes. e.g the episode &lt;em&gt;The Rains of Castamere&lt;/em&gt; in the TV series &lt;em&gt;Game of Thrones&lt;/em&gt; had volcanic effect on the public sentiment. 
Also we can look at the geocoded information in the tweets and analyze the relation between location and mood. 
&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Techniques:&lt;/strong&gt; There are broadly two categories of sentiment analysis: 
&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Lexical Methods:&lt;/strong&gt;
&lt;br&gt; These techniques employ dictionaries of words annotated with their semantic polarity and sentiment strength. This is then used to calculate a score for the polarity and/or sentiment of the document. Usually this method gives high precision but low recall.
&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Machine Learning Methods:&lt;/strong&gt;
&lt;br&gt; Such techniques require creating a model by training the classifier with labeled examples. This means that you must first gather a dataset with examples for positive, negative and neutral classes, extract the features from the examples and then train the algorithm based on the examples. These methods are used mainly for computing the polarity of the document. 
&lt;br&gt;&lt;br&gt;
Choice of the method heavily depends on the application, domain and language. Using lexicon based techniques with large dictionaries enables us to achieve very good results. Nevertheless they require using a lexicon, something which is not always available in all languages. 
On the other hand Machine Learning based techniques deliver good results but they require obtaining training on labeled data.
&lt;br&gt;&lt;br&gt;
Now I&amp;#39;ll discuss an example of each of the above techniques. 
&lt;br&gt;&lt;br&gt;
&lt;strong&gt;AFINN Model:&lt;/strong&gt; 
&lt;br&gt;
In the &lt;a href=&quot;http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010&quot;&gt;AFINN model&lt;/a&gt;, the authors have computed sentiment scores for a list of words. The sentiment of a tweet is computed based on the sentiment scores of the terms in the tweet. The sentiment of the tweet is defined to be equal to the sum of the sentiment scores for each term in the tweet. The AFINN-111 dictionary contains 2477 English words rated for valence with an integer value between -5 and 5. The words have been manually labelled by Finn Arup Neilsen in 2009-2010. Some of the words are the grammatically different versions of the same stem e.g. &amp;quot;favorite&amp;quot; and &amp;quot;favorites&amp;quot; are listed as two different words with different valence scores. 
&lt;br&gt;
An implementation of the AFINN model can be &lt;a href=&quot;https://github.com/Jverma/Twitter-Sentiment-Analysis&quot;&gt;found here&lt;/a&gt;.
&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Naive Bayes Classifier:&lt;/strong&gt;
&lt;br&gt;
The &lt;a href=&quot;http://en.wikipedia.org/wiki/Naive_Bayes_classifier&quot;&gt;Naive Bayes classifier&lt;/a&gt;can be trained on a corpus of labeled (+ve, -ve, neutral) tweets and then employed to assign polarity to a new tweet. The features used in this model are the words or bi-grams with their frequencies in the tweet strings. You may want to keep or remove URLs, emoticons and short tokens depending on the application. 
&lt;br&gt;&lt;br&gt;
If \( f = (f_{1}, f_{2}, \ldots, f_{n}) \) be the vector of features of a tweet ( T ), then the probability of the tweet to be +ve according to Bayes Rule is:&lt;br&gt;
$$ p(+ve|f ) = ( p(f|+ve) * p(+ve) ) / p(f) $$
&lt;br&gt;
Now basic theory of probability tells us that: 
&lt;br&gt;
$$
p(f|+ve) =  p(f_{1}, f_{2}, \ldots, f_{n} | +ve)
$$
&lt;br&gt;
and 
&lt;br&gt;
$$ p(f) = p(f_{1}, f_{2}, \ldots, f_{n}) $$
&lt;br&gt;
The Naive Bayes asserts that all the features are independent and thus: 
&lt;br&gt;
$$ p(f) = \prod_{i} p(f_{i}) $$
&lt;br&gt;
$$ p(f|+ve) = \prod_{i} p(f_{i}|+ve) $$
&lt;br&gt;
And similarly for -ve and neutral tweets. 
&lt;br&gt;&lt;br&gt;
Using the pre-estimated values of these probabilities, one can compute the probability of a tweet to be positive, negative and neutral. 
Whenever a new tweet is fed to the classifier, it will predict the polarity of the tweet based on the probability of its having that polarity. 
&lt;br&gt;
An implementation of Naive Bayes classifier for classifying spam and non-spam messages can be found &lt;a href=&quot;https://github.com/Jverma/naive-bayes&quot;&gt;here&lt;/a&gt;. 
&lt;br&gt;&lt;br&gt;
Several other methods in both the categories are prevalent today. Lots of companies using sentiment analysis employ lexical methods where they create dictionaries based on their trade algorithms and the domain of the application. 
&lt;br&gt;
For machine learning based analysis, instead of Naive Bayes, one can use more sophisticated algorithms like SVMs. 
&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Challenges:&lt;/strong&gt;
&lt;br&gt;
Sentiment analysis is a very useful, but there are many challenges that need to be overcome to achieve good results. The very first step in opinion mining, something which I swept under the rug so far, is that we have to identify tweets that are relevant to our topic. Tweets containing the given word can be a decent choice, although not perfect. Once we have identified tweets to be analyzed, we need to sure that the tweets DO contain sentiment. Neutral tweets can be a part of our model, but only polarized tweets tell us something subjective. Even though the tweets are polarized, we still need to make sure that the sentiment in the tweet is related to the topic we are studying. For example, suppose we are studying sentiment related to a movie Mission Impossible, then the tweet: “Tom Cruise in Mission Impossible is pathetic!”.
&lt;p&gt;
Now this tweet has a negative sentiment, but is directed at the actor rather than the movie. This is not a great example, as the sentiment of the actor and movie is related.
&lt;br&gt;
The main challenge in Sentiment analysis using lexical methods is to build a dictionary that contains words/phrases and their sentiment scores. It is very hard to do so in full generality, and often the best idea is to choose a subject and build a list for that. Thus sentiment analysis is highly domain centric, so the techniques developed for stocks may not work for movies.
&lt;br&gt;
To solve these problems, you need expertise in NLP and computational linguistics. They correspond to entity extraction, NER, and entity pattern extraction in NLP terminology.
&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Beyond Twitter:&lt;/strong&gt;
&lt;br&gt;
Facebook performed an experiment to measure the effect of removing positive (or negative) posts from the people&amp;#39;s news feeds on how positive (or negative) their own posts were in the days after these changes were made. They found that the people from whose news feeds negative posts were removed produced a larger percentage of positive words as well as a smaller percentage of negative words in their posts. The group of people from whose news feeds negative posts were removed showed similar tendencies. The procedure and results of this experiment were a paper in the Proceedings of the National Academy of Sciences. Though, I don’t subscribe to the idea of using users are subjects to a physiological experiment without their knowledge, this is a cool application of sentiment analysis subject area.
&lt;br&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Resources:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Liu, Bing. &amp;quot;Sentiment analysis and subjectivity.&amp;quot; Handbook of natural language processing 2 (2010): 627-666.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Liu, Bing, and Lei Zhang. &amp;quot;A survey of opinion mining and sentiment analysis.&amp;quot;Mining Text Data. Springer US, 2012. 415-463.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Liu, Bing. &amp;quot;Sentiment analysis and opinion mining.&amp;quot; Synthesis Lectures on Human Language Technologies 5.1 (2012): 1-167.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.cs.cornell.edu/home/llee/opinion-mining-sentiment-analysis-survey.html&quot;&gt;Opinion mining and sentiment analysis&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/&quot;&gt;Twitter sentiment analysis using Python and NLTK&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

    &lt;p&gt;&lt;a href=&quot;http://jverma.github.io//twitter-sentiment-analysis/&quot;&gt;Twitter Sentiment Analysis&lt;/a&gt; was originally published by Janu Verma at &lt;a href=&quot;http://jverma.github.io/&quot;&gt;Random Inferences&lt;/a&gt; on February 24, 2015.&lt;/p&gt;
  </content>
</entry>


  

<entry>
  <title type="html"><![CDATA[Linear Regression : Frequentist and Bayesian]]></title>
  <link rel="alternate" type="text/html" href="http://jverma.github.io//regression-freq-bayesian/" />
  <id>http://jverma.github.io//regression-freq-bayesian</id>
  <published>2015-02-03T00:00:00-05:00</published>
  <updated>2015-02-03T00:00:00-05:00</updated>
  <author>
    <name>Janu Verma</name>
    <uri>http://jverma.github.io/</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;We often hear there are two schools of thought in statistics - Frequentist and Bayesian. A the very fundamental level the difference in these two approaches stems from the way they interpret probability. For a frequentist, probability is defined in terms of limiting frequency of occurrence of an event while a bayesian statistician defines probability as the degree of disbelief on the occurrence of an event. This post in not about the philosophical aspects of the debate. Rather we will study an example from frequentist and bayesian methods. The example we will consider is the linear regression model. 
&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Setup:&lt;/strong&gt; 
&lt;br&gt;
Let the data be \( D = ( x_{i} , y_{i} )_{1 \leq i \leq N} \) where each \( x_{i}  \in \mathbb{R}^n \) and \( y_{i} \in \mathbb{R} \).
The linear regression model predicts the values of \( y_{i} \)&amp;#39;s as linear combinations of the features \( x_{i} \)&amp;#39;s&lt;/p&gt;

&lt;p&gt;$$ y_{i} = w_{0} + \sum_{j} w_{j} x_{ij} = w_{0} +w^T x_{i} $$&lt;/p&gt;

&lt;p&gt;Adding 1 to the vectors \(( x_{i} \)) to redefine \(( x_i = (1,x_i) \)) and combining \(( w_0 \)) and \(( w_i \))&amp;#39;s into a single vector \(( w = (w_0, w_1, \ldots, w_n) \)), this can be written as &lt;/p&gt;

&lt;p&gt;$$ y_i = w^T x_i $$&lt;/p&gt;

&lt;p&gt;The idea is to estimate the values of parameters \(( \hat{w} \)) from the training data and predict the \(( y \))-value for a new observation \(( \tilde{x} \)) as &lt;/p&gt;

&lt;p&gt;$$ \tilde{y} = \hat{w}^T \tilde{x} $$
&lt;br&gt;&lt;br&gt;
We will consider &lt;em&gt;Maximum likelihood estimation&lt;/em&gt; (Frequentist), &lt;em&gt;Maximum a Posteriori&lt;/em&gt; (semi-bayesian) and &lt;em&gt;Bayesian regression models&lt;/em&gt;. 
&lt;br&gt;&lt;br&gt;
&lt;strong&gt;tl;dr&lt;/strong&gt;
&lt;br&gt;
MLE chooses the parameters which maximize the likelihood of data given that parameter, MAP chooses parameters which maximize the posterior probability  of that parameter in the light of observed data and Bayesian inference computes the posterior probability distribution for the parameters given data.
&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Maximum Likelihood Estimation:&lt;/strong&gt;
&lt;br&gt;
The observed values of \(( y_i \))  is assumed to have Gaussian noise error i.e. &lt;/p&gt;

&lt;p&gt;$$ y_i = w^T x_i + \epsilon $$&lt;/p&gt;

&lt;p&gt;where \(( \epsilon \sim N(0,\sigma^2) \)).
&lt;br&gt;
The Likelihood in this case is given by
&lt;br&gt;
$$ \mathcal{L}(D | w, \sigma) = (2 \pi \sigma^2)^{-n/2} \prod_{i=1}^{n} exp \left[ \frac{-(y_i - \hat{y}_i)^2}{2\sigma^2} \right] $$
&lt;br&gt;
then the log-likelihood is given by
&lt;br&gt;
$$ \ln(\mathcal{L}) = - \frac{n}{2} \ln(2\pi \sigma^2) -  \frac{1}{2\sigma^2} \sum_i (y_i - w^T x_i)^2 $$
&lt;br&gt;
Now MLE states that the estimated value of \(( w \)) is given by
&lt;br&gt;
$$w_{MLE} = argmax_w \ln(\mathcal{L}(D | w, \sigma)) $$
&lt;br&gt;
which in this case reduces to
&lt;br&gt;
$$ w_{MLE} = argmin_w \sum_i  (y_i - w^T x_i)^2 $$
&lt;br&gt;&lt;br&gt;
This is why the linear regression model is often known as &lt;em&gt;least square method&lt;/em&gt;. 
Now we differentiate with respect to \(( w \)) and equate the derivative to zero to get the estimate of \(( w \)). This can be more clearly expressed in terms of linear algebraic quantities.
 &lt;br&gt;&lt;br&gt;
If \(( X = (x_1, x_2, \ldots, x_N)^T \)), \(( Y = (y_1, y_2, \ldots, y_N)^T \)) and  \(( \theta = (w_0, w_1, w_2, \ldots, w_N)^T \)), then one can check that the MLE for \(( \theta \)) is 
&lt;br&gt;
$$ \hat{\theta} = (X^T X)^{-1} X^T Y $$
&lt;br&gt;
Similarly  \(( \sigma^2 \)) can be estimated by differentiating the MLE with respect to  \(( \sigma^2 \)) and equation the derivative to zero. 
&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Maximum a Posteriori estimation:&lt;/strong&gt;
&lt;br&gt;
 For MAP, we assume a Gaussian prior on \(( w \)) i.e 
$$ w \sim N(0, \lambda^{-1} I)$$
&lt;br&gt;
$$ P(w) = (\frac{\lambda}{2 \pi})^{n/2} exp \left[ -\frac{\lambda}{2} w^T w \right] $$
&lt;br&gt;
Then the posterior probability after we observe the training data is computed by Bayes rule as &lt;/p&gt;

&lt;p&gt;$$ P(w|D) = \frac{P(w) P(D|w)}{P(D)}$$
&lt;br&gt;
as with MLE, we will maximize the log-posterior probability and then 
&lt;br&gt;
$$ w_{MAP} = argmax_w \ln(P(w|d)) $$
&lt;br&gt;
which reduces to&lt;/p&gt;

&lt;p&gt;$$ w_{MAP} = argmin_w \sum_i (y_i - w^T x_i)^2 + \frac{\lambda}{2} w^T w $$
&lt;br&gt;
Thus, the MAP estimation can be thought of as regression with regularization. 
The MAP estimate in this case is 
&lt;br&gt;
$$ w_{MAP} = (\lambda I + X^T X)^{-1} X^T y $$&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;br&gt;
&lt;strong&gt;Bayesian Regression:&lt;/strong&gt;
&lt;br&gt;
 In Bayesian regression, the Bayesian philosophy is applied. Both MLE and MAP are point estimates but in Bayesian regression, we look for predictive probability. Here we make predictions by integrating over the posterior distribution of the model parameters \(( w \)). 
If \(( \tilde{x} \)) is a new point, we compute the probability of \(( \tilde{y} \)), the y-value corresponding to this x is given by
&lt;br&gt;
$$ P(\tilde{y} | \tilde{x}, D, \sigma^2, \lambda) = \int P(\tilde{y} | w, \tilde{x}, \sigma^2 ) P(w|D, \sigma^2 , \lambda) dw $$
&lt;br&gt;
These integration are often very hard to to do analytically and rely on sophisticated MCMC methods. 
 &lt;br&gt;&lt;br&gt;
In full Bayesian regression, we assume a prior on \(( \sigma^2 \)) in addition to prior on \(( w \)).&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;http://jverma.github.io//regression-freq-bayesian/&quot;&gt;Linear Regression : Frequentist and Bayesian&lt;/a&gt; was originally published by Janu Verma at &lt;a href=&quot;http://jverma.github.io/&quot;&gt;Random Inferences&lt;/a&gt; on February 03, 2015.&lt;/p&gt;
  </content>
</entry>

</feed>
